{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "import logging\n",
    "logging.basicConfig(filename='logs.txt',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "# local package\n",
    "from gptfhir.azureLLM import AzureLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/gpt/lib/python3.11/site-packages/langchain/llms/openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniconda/base/envs/gpt/lib/python3.11/site-packages/langchain/llms/openai.py:801: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get the config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "# environment var\n",
    "os.environ['OPENAI_API_KEY'] = config['AZURE']['AZURE_API_KEY']\n",
    "\n",
    "# get the Azure LLM object\n",
    "azure_llm = AzureLLM(config)\n",
    "\n",
    "# get test data\n",
    "with open('../data/test_input.txt', 'r') as in_file:\n",
    "    test_input = in_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml_runtime import SchemaView\n",
    "import pickle\n",
    "\n",
    "# load template\n",
    "path_to_template = 'test.yaml'\n",
    "sv = SchemaView(path_to_template)\n",
    "\n",
    "def get_completion_prompt(cls_name, sv, text):\n",
    "    \"\"\"Get the prompt for the given template.\"\"\"\n",
    "\n",
    "    # system prompt\n",
    "    prompt = (\n",
    "        \"From the text below, extract the following entities in the following format:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # schema prompt is concatinated from prompts for each entity group\n",
    "    for slot in sv.class_induced_slots(cls_name):\n",
    "        \n",
    "        # description of the entity group\n",
    "        slot_prompt = slot.description\n",
    "\n",
    "        # custom instruction for categorical entities\n",
    "        if slot.range in sv.all_enums():\n",
    "            enum_def = sv.get_enum(slot.range)\n",
    "            pvs = [str(k) for k in enum_def.permissible_values.keys()]\n",
    "            slot_prompt += f\"Must be one of: {', '.join(pvs)}\"\n",
    "\n",
    "        # entity group prompt\n",
    "        prompt += f\"{slot.name}: <{slot_prompt}>\\n\"\n",
    "    \n",
    "    # concatinate the prompt with doctor's note\n",
    "    prompt = f\"{prompt}\\n\\nText:\\n{text}\\n\\n===\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "# generate the prompt\n",
    "prompt = get_completion_prompt(\n",
    "    cls_name='ClinicalNote',\n",
    "    sv=sv,\n",
    "    text = test_input\n",
    ")\n",
    "\n",
    "# save up on API calls :)\n",
    "rerun = False\n",
    "if rerun:\n",
    "    \n",
    "    # LLM call to extract entities \n",
    "    result = azure_llm.llm(prompt)\n",
    "\n",
    "    # Dump results\n",
    "    with open('../data/test_results.pkl', 'wb') as file:\n",
    "        pickle.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             ann[field] \u001b[39m=\u001b[39m val\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m ann\n\u001b[0;32m---> 28\u001b[0m ann \u001b[39m=\u001b[39m parse_response_to_dict(\n\u001b[1;32m     29\u001b[0m     result \u001b[39m=\u001b[39;49m result, \n\u001b[1;32m     30\u001b[0m     cls_name \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mClinicalNote\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     31\u001b[0m     sv \u001b[39m=\u001b[39;49m sv\n\u001b[1;32m     32\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mparse_response_to_dict\u001b[0;34m(result, cls_name, sv)\u001b[0m\n\u001b[1;32m     20\u001b[0m         logging\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLine \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mline\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not contain a colon; ignoring\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39m_parse_line_to_dict(line, \u001b[39mcls\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     field, val \u001b[39m=\u001b[39m r\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/test_results.pkl', 'rb') as file:\n",
    "    result = pickle.load(file)\n",
    "\n",
    "def _parse_line_to_dict(line, sv, cls_name):\n",
    "    logging.info(f\"PARSING LINE: {line}\")\n",
    "    field, val = line.split(\":\", 1)\n",
    "    # Field nornalization:\n",
    "    # The LLML may mutate the output format somewhat,\n",
    "    # randomly pluralizing or replacing spaces with underscores\n",
    "    field = field.lower().replace(\" \", \"_\")\n",
    "    cls_slots = sv.class_slots(cls.name)\n",
    "    slot = None\n",
    "    if field in cls_slots:\n",
    "        slot = sv.induced_slot(field, cls.name)\n",
    "    else:\n",
    "        if field.endswith(\"s\"):\n",
    "            field = field[:-1]\n",
    "        if field in cls_slots:\n",
    "            slot = sv.induced_slot(field, cls.name)\n",
    "    if not slot:\n",
    "        logging.error(f\"Cannot find slot for {field} in {line}\")\n",
    "        # raise ValueError(f\"Cannot find slot for {field} in {line}\")\n",
    "        return\n",
    "    if not val:\n",
    "        msg = f\"Empty value in key-value line: {line}\"\n",
    "        if slot.required:\n",
    "            raise ValueError(msg)\n",
    "        if slot.recommended:\n",
    "            logging.warning(msg)\n",
    "        return\n",
    "    inlined = slot.inlined\n",
    "    slot_range = sv.get_class(slot.range)\n",
    "    if not inlined:\n",
    "        if slot.range in sv.all_classes():\n",
    "            inlined = sv.get_identifier_slot(slot_range.name) is None\n",
    "    val = val.strip()\n",
    "    if slot.multivalued:\n",
    "        vals = [v.strip() for v in val.split(\";\")]\n",
    "    else:\n",
    "        vals = [val]\n",
    "    vals = [val for val in vals if val]\n",
    "    logging.debug(f\"SLOT: {slot.name} INL: {inlined} VALS: {vals}\")\n",
    "    if inlined:\n",
    "        transformed = False\n",
    "        slots_of_range = sv.class_slots(slot_range.name)\n",
    "        if len(slots_of_range) > 2:\n",
    "            vals = [self._extract_from_text_to_dict(v, slot_range) for v in vals]\n",
    "        else:\n",
    "            for sep in [\" - \", \":\", \"/\", \"*\", \"-\"]:\n",
    "                if all([sep in v for v in vals]):\n",
    "                    vals = [dict(zip(slots_of_range, v.split(sep, 1))) for v in vals]\n",
    "                    for v in vals:\n",
    "                        for k in v.keys():\n",
    "                            v[k] = v[k].strip()\n",
    "                    transformed = True\n",
    "                    break\n",
    "            if not transformed:\n",
    "                logging.warning(f\"Did not find separator in {vals} for line {line}\")\n",
    "                return\n",
    "    # transform back from list to single value if not multivalued\n",
    "    if slot.multivalued:\n",
    "        final_val = vals\n",
    "    else:\n",
    "        if len(vals) != 1:\n",
    "            logging.error(f\"Expected 1 value for {slot.name} in '{line}' but got {vals}\")\n",
    "        final_val = vals[0]\n",
    "    return field, final_val\n",
    "\n",
    "def parse_response_to_dict(result, cls_name, sv):\n",
    "    lines = result.splitlines()\n",
    "    ann = {}\n",
    "    promptable_slots = sv.class_induced_slots(cls_name)\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \":\" not in line:\n",
    "            if len(promptable_slots) == 1:\n",
    "                slot = promptable_slots[0]\n",
    "                logging.warning(\n",
    "                    f\"Coercing to YAML-like with key {slot.name}: Original line: {line}\"\n",
    "                )\n",
    "                line = f\"{slot.name}: {line}\"\n",
    "            else:\n",
    "                logging.error(f\"Line '{line}' does not contain a colon; ignoring\")\n",
    "                return\n",
    "        r = parse_line_to_dict(line, cls_name)\n",
    "        if r is not None:\n",
    "            field, val = r\n",
    "            ann[field] = val\n",
    "    return ann\n",
    "\n",
    "def _extract_from_text_to_dict(self, text: str, cls: ClassDefinition = None) -> RESPONSE_DICT:\n",
    "    raw_text = self._raw_extract(text, cls)\n",
    "    return parse_response_to_dict(raw_text, cls)\n",
    "\n",
    "ann = parse_response_to_dict(\n",
    "    result = result, \n",
    "    cls_name = 'ClinicalNote', \n",
    "    sv = sv\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
