{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: http://localhost:7860/ âœ”\n"
     ]
    }
   ],
   "source": [
    "# logger\n",
    "import logging\n",
    "logging.basicConfig(filename='logs.txt',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import ast\n",
    "from fhirclient.client import FHIRClient\n",
    "import configparser\n",
    "import gradio_client\n",
    "import fhirclient.models.observation as o\n",
    "import fhirclient.models.annotation as a\n",
    "from linkml_runtime import SchemaView\n",
    "import pickle\n",
    "\n",
    "# get the config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "# environment var\n",
    "os.environ['OPENAI_API_KEY'] = config['AZURE']['AZURE_API_KEY']\n",
    "\n",
    "# connection to FHIR server\n",
    "settings = {\n",
    "    'app_id': 'my_web_app',\n",
    "    'api_base': 'http://localhost:8080/fhir/'\n",
    "}\n",
    "fhir_client = FHIRClient(settings=settings)\n",
    "\n",
    "# connection to Gradlio server\n",
    "HOST_URL = \"http://localhost:7860\"\n",
    "llm_client = gradio_client.Client(HOST_URL)\n",
    "\n",
    "# get observations\n",
    "search = o.Observation.where(struct={'status': 'final'})\n",
    "observations = search.perform_resources(fhir_client.server)\n",
    "\n",
    "# load LinkML template\n",
    "path_to_template = 'test.yaml'\n",
    "sv = SchemaView(path_to_template)\n",
    "\n",
    "# llm call function\n",
    "def run_llm(prompt):\n",
    "\n",
    "    # string of dict for input\n",
    "    kwargs = dict(instruction_nochat=prompt)\n",
    "    res = llm_client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n",
    "\n",
    "    # string of dict for output\n",
    "    return ast.literal_eval(res)['response']\n",
    "\n",
    "def get_completion_prompt(cls_name, sv, text):\n",
    "    \"\"\"Get the prompt for the given template.\"\"\"\n",
    "\n",
    "    # system prompt\n",
    "    prompt = (\n",
    "        \"From the text below, extract the following entities in the following format (if available):\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # schema prompt is concatinated from prompts for each entity group\n",
    "    for slot in sv.class_induced_slots(cls_name):\n",
    "        \n",
    "        # description of the entity group\n",
    "        slot_prompt = slot.description\n",
    "\n",
    "        # custom instruction for categorical entities\n",
    "        if slot.range in sv.all_enums():\n",
    "            enum_def = sv.get_enum(slot.range)\n",
    "            pvs = [str(k) for k in enum_def.permissible_values.keys()]\n",
    "            slot_prompt += f\"Must be one of: {', '.join(pvs)}\"\n",
    "\n",
    "        # entity group prompt\n",
    "        prompt += f\"{slot.name}: <{slot_prompt}>\\n\"\n",
    "    \n",
    "    # concatinate the prompt with doctor's note\n",
    "    prompt = f\"{prompt}\\n\\nText:\\n{text}\\n\\n===\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def annotate_observation(observation):\n",
    "    \n",
    "    # generate the prompt\n",
    "    prompt = get_completion_prompt(\n",
    "        cls_name='ClinicalNote',\n",
    "        sv=sv,\n",
    "        text = observation.code.text\n",
    "    )\n",
    "    \n",
    "    # LLM call to extract entities \n",
    "    result = run_llm(prompt)\n",
    "    \n",
    "    # update observation\n",
    "    observation.note = [a.Annotation({\n",
    "        \"authorString\" : \"Annotated by LLama2\",\n",
    "        \"text\": result\n",
    "    })]\n",
    "    observation.update(fhir_client.server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate all observations\n",
    "for observation in observations:\n",
    "    annotate_observation(observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<fhirclient.models.observation.Observation at 0x10edf52d0>,\n",
       " <fhirclient.models.observation.Observation at 0x110289150>,\n",
       " <fhirclient.models.observation.Observation at 0x110289550>,\n",
       " <fhirclient.models.observation.Observation at 0x1102898d0>,\n",
       " <fhirclient.models.observation.Observation at 0x110298e10>,\n",
       " <fhirclient.models.observation.Observation at 0x1102993d0>,\n",
       " <fhirclient.models.observation.Observation at 0x11029ac90>,\n",
       " <fhirclient.models.observation.Observation at 0x110298510>,\n",
       " <fhirclient.models.observation.Observation at 0x110298110>,\n",
       " <fhirclient.models.observation.Observation at 0x110298a10>,\n",
       " <fhirclient.models.observation.Observation at 0x110299a90>,\n",
       " <fhirclient.models.observation.Observation at 0x11029b350>,\n",
       " <fhirclient.models.observation.Observation at 0x11029ad10>,\n",
       " <fhirclient.models.observation.Observation at 0x110298e90>,\n",
       " <fhirclient.models.observation.Observation at 0x110237510>,\n",
       " <fhirclient.models.observation.Observation at 0x110235c50>,\n",
       " <fhirclient.models.observation.Observation at 0x110237990>,\n",
       " <fhirclient.models.observation.Observation at 0x11026de10>,\n",
       " <fhirclient.models.observation.Observation at 0x11026ce50>,\n",
       " <fhirclient.models.observation.Observation at 0x11026ccd0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/observations_with_annotations.pkl', 'wb') as file:\n",
    "    pickle.dump(observations_as_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_as_json = [a.as_json() for a in observations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/observations_with_annotations.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> 2\u001b[0m     test \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      3\u001b[0m test\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/observations_with_annotations.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# FROM ACCENTURE #\n",
    "##################\n",
    "\n",
    "# logger\n",
    "import logging\n",
    "logging.basicConfig(filename='logs.txt',\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "# imports\n",
    "import pickle\n",
    "import fhirclient.models.observation as o\n",
    "from linkml_runtime import SchemaView\n",
    "\n",
    "# load data\n",
    "with open('../data/observations_with_annotations.pkl', 'rb') as file:\n",
    "    observations_as_json = pickle.load(file)\n",
    "observations = [o.Observation(a) for a in observations_as_json]\n",
    "\n",
    "# load LinkML template\n",
    "path_to_template = 'test.yaml'\n",
    "sv = SchemaView(path_to_template)\n",
    "\n",
    "# select test case\n",
    "test_observation = observations[0]\n",
    "ann_text = test_observation.note[0].text\n",
    "\n",
    "# delete intro\n",
    "ann_text = ann_text[ann_text.find('age'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:PARSING LINE: age: Not available (as it is not mentioned in the text)\n",
      "DEBUG:root:SLOT: age INL: None VALS: ['Not available (as it is not mentioned in the text)']\n",
      "INFO:root:PARSING LINE: gender: Male (as the patient's name is \"Mr. John Doe\")\n",
      "DEBUG:root:SLOT: gender INL: None VALS: ['Male (as the patient\\'s name is \"Mr. John Doe\")']\n",
      "INFO:root:PARSING LINE: conditions:\n",
      "ERROR:root:Line '* Diabetes management' does not contain a colon; ignoring\n"
     ]
    }
   ],
   "source": [
    "def _parse_line_to_dict(line, sv, cls_name):\n",
    "    logging.info(f\"PARSING LINE: {line}\")\n",
    "    field, val = line.split(\":\", 1)\n",
    "    # Field nornalization:\n",
    "    # The LLML may mutate the output format somewhat,\n",
    "    # randomly pluralizing or replacing spaces with underscores\n",
    "    field = field.lower().replace(\" \", \"_\")\n",
    "    cls_slots = sv.class_slots(cls_name)\n",
    "    slot = None\n",
    "    if field in cls_slots:\n",
    "        slot = sv.induced_slot(field, cls_name)\n",
    "    else:\n",
    "        if field.endswith(\"s\"):\n",
    "            field = field[:-1]\n",
    "        if field in cls_slots:\n",
    "            slot = sv.induced_slot(field, cls_name)\n",
    "    if not slot:\n",
    "        logging.error(f\"Cannot find slot for {field} in {line}\")\n",
    "        # raise ValueError(f\"Cannot find slot for {field} in {line}\")\n",
    "        return\n",
    "    if not val:\n",
    "        msg = f\"Empty value in key-value line: {line}\"\n",
    "        if slot.required:\n",
    "            raise ValueError(msg)\n",
    "        if slot.recommended:\n",
    "            logging.warning(msg)\n",
    "        return\n",
    "    inlined = slot.inlined\n",
    "    slot_range = sv.get_class(slot.range)\n",
    "    if not inlined:\n",
    "        if slot.range in sv.all_classes():\n",
    "            inlined = sv.get_identifier_slot(slot_range.name) is None\n",
    "    val = val.strip()\n",
    "    if slot.multivalued:\n",
    "        vals = [v.strip() for v in val.split(\";\")]\n",
    "    else:\n",
    "        vals = [val]\n",
    "    vals = [val for val in vals if val]\n",
    "    logging.debug(f\"SLOT: {slot.name} INL: {inlined} VALS: {vals}\")\n",
    "    if inlined:\n",
    "        transformed = False\n",
    "        slots_of_range = sv.class_slots(slot_range.name)\n",
    "        if len(slots_of_range) > 2:\n",
    "            vals = [_extract_from_text_to_dict(v, slot_range) for v in vals]\n",
    "        else:\n",
    "            for sep in [\" - \", \":\", \"/\", \"*\", \"-\"]:\n",
    "                if all([sep in v for v in vals]):\n",
    "                    vals = [dict(zip(slots_of_range, v.split(sep, 1))) for v in vals]\n",
    "                    for v in vals:\n",
    "                        for k in v.keys():\n",
    "                            v[k] = v[k].strip()\n",
    "                    transformed = True\n",
    "                    break\n",
    "            if not transformed:\n",
    "                logging.warning(f\"Did not find separator in {vals} for line {line}\")\n",
    "                return\n",
    "    # transform back from list to single value if not multivalued\n",
    "    if slot.multivalued:\n",
    "        final_val = vals\n",
    "    else:\n",
    "        if len(vals) != 1:\n",
    "            logging.error(f\"Expected 1 value for {slot.name} in '{line}' but got {vals}\")\n",
    "        final_val = vals[0]\n",
    "    return field, final_val\n",
    "\n",
    "def parse_response_to_dict(result, cls_name, sv):\n",
    "    lines = result.splitlines()\n",
    "    ann = {}\n",
    "    promptable_slots = sv.class_induced_slots(cls_name)\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if \":\" not in line:\n",
    "            if len(promptable_slots) == 1:\n",
    "                slot = promptable_slots[0]\n",
    "                logging.warning(\n",
    "                    f\"Coercing to YAML-like with key {slot.name}: Original line: {line}\"\n",
    "                )\n",
    "                line = f\"{slot.name}: {line}\"\n",
    "            else:\n",
    "                logging.error(f\"Line '{line}' does not contain a colon; ignoring\")\n",
    "                return\n",
    "        r = _parse_line_to_dict(line, sv, cls_name)\n",
    "        if r is not None:\n",
    "            field, val = r\n",
    "            ann[field] = val\n",
    "    return ann\n",
    "\n",
    "def _extract_from_text_to_dict(raw_text, cls_name):\n",
    "    return parse_response_to_dict(raw_text, cls_name)\n",
    "\n",
    "ann = parse_response_to_dict(\n",
    "    result = ann_text, \n",
    "    cls_name = 'ClinicalNote', \n",
    "    sv = sv\n",
    ")\n",
    "\n",
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             ann[field] \u001b[39m=\u001b[39m val\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m ann\n\u001b[0;32m---> 28\u001b[0m ann \u001b[39m=\u001b[39m parse_response_to_dict(\n\u001b[1;32m     29\u001b[0m     result \u001b[39m=\u001b[39;49m result, \n\u001b[1;32m     30\u001b[0m     cls_name \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mClinicalNote\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     31\u001b[0m     sv \u001b[39m=\u001b[39;49m sv\n\u001b[1;32m     32\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mparse_response_to_dict\u001b[0;34m(result, cls_name, sv)\u001b[0m\n\u001b[1;32m     20\u001b[0m         logging\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLine \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mline\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not contain a colon; ignoring\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39m_parse_line_to_dict(line, \u001b[39mcls\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     field, val \u001b[39m=\u001b[39m r\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/test_results.pkl', 'rb') as file:\n",
    "    result = pickle.load(file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
